# Sistema RAG - BGE-M3 + ChromaDB + Groq/DeepSeek

Sistema completo de RecuperaciÃ³n Aumentada por GeneraciÃ³n (RAG) que utiliza:
- **BGE-M3** para generar embeddings semÃ¡nticos (1024 dimensiones)
- **ChromaDB** para almacenamiento vectorial con HNSW
- **Groq API** (ultra-rÃ¡pido, recomendado, Llama 3.3 70B) o **DeepSeek API** como modelo de lenguaje

## ğŸ“‹ CaracterÃ­sticas

- âœ… Procesamiento de documentos Markdown (.md)
- âœ… GeneraciÃ³n de embeddings con BGE-M3
- âœ… Almacenamiento vectorial en ChromaDB (sin configuraciÃ³n, persistencia automÃ¡tica)
- âœ… BÃºsqueda semÃ¡ntica con similitud coseno y HNSW
- âœ… **Groq API con Llama 3.3 70B** (ultra-rÃ¡pido, 10-20x mÃ¡s rÃ¡pido que alternativas)
- âœ… Alternativa DeepSeek API
- âœ… **Chatbot interactivo por consola** con historial de conversaciÃ³n
- âœ… Modo de consultas Ãºnicas CLI
- âœ… DivisiÃ³n opcional de documentos en chunks
- âœ… Manejo robusto de errores

## ğŸ—ï¸ Estructura del Proyecto

```
LLAMA_BGE_CHATBOT/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ docs/              # Archivos .md para ingestion
â”‚   â””â”€â”€ chroma/            # Base de datos ChromaDB (auto-generado)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ embedder.py    # GeneraciÃ³n de embeddings BGE-M3
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ chroma_vector_store.py  # ChromaDB storage
â”‚   â”‚   â””â”€â”€ repository.py  # Operaciones CRUD
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ ingest_docs.py # Carga y preprocesamiento
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ retriever.py   # BÃºsqueda semÃ¡ntica
â”‚   â”‚   â””â”€â”€ rag_pipeline.py # Pipeline completo
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ groq_client.py      # Cliente Groq API (recomendado)
â”‚   â”‚   â””â”€â”€ deepseek_client.py  # Cliente DeepSeek API
â”‚   â”œâ”€â”€ chatbot/
â”‚   â”‚   â””â”€â”€ chatbot.py     # Chatbot con historial
â”‚   â”œâ”€â”€ chat.py            # Chatbot interactivo de consola
â”‚   â””â”€â”€ main.py            # Punto de entrada CLI
â”‚
â”œâ”€â”€ .env.example           # Template de variables de entorno
â”œâ”€â”€ requirements.txt       # Dependencias
â”œâ”€â”€ README.md              # Esta documentaciÃ³n
â””â”€â”€ CLAUDE.md              # GuÃ­a para Claude Code
```

## ğŸš€ InstalaciÃ³n

### 1. Clonar o descargar el proyecto

```bash
git clone <tu-repo>
cd LLAMA_BGE_CHATBOT
```

### 2. Crear entorno virtual

```bash
# Linux/Mac
python -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
venv\Scripts\activate
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Configurar variables de entorno

Copia el archivo `.env.example` a `.env` y configura tu API key:

```bash
cp .env.example .env
```

Edita `.env` con tu API key (solo necesitas una):

```env
# Groq API (ultra-rÃ¡pido, 14,400 requests/dÃ­a gratis) - RECOMENDADO
GROQ_API_KEY=tu_groq_api_key_aqui

# DeepSeek API (alternativa mÃ¡s lenta pero buena calidad)
DEEPSEEK_API_KEY=tu_deepseek_api_key_aqui
```

**Obtener API Key de Groq** (Recomendado - Ultra RÃ¡pido âš¡):
1. Visita [https://console.groq.com/](https://console.groq.com/)
2. Crea una cuenta gratuita
3. Ve a API Keys
4. Genera una nueva API key
5. CÃ³piala en el archivo `.env` como `GROQ_API_KEY`

**Obtener API Key de DeepSeek** (Alternativa):
1. Visita [https://platform.deepseek.com/](https://platform.deepseek.com/)
2. Crea una cuenta o inicia sesiÃ³n
3. Ve a la secciÃ³n de API Keys
4. Genera una nueva API key
5. CÃ³piala en el archivo `.env` como `DEEPSEEK_API_KEY`

**ComparaciÃ³n de LLMs:**

| CaracterÃ­stica | Groq âš¡ (Recomendado) | DeepSeek |
|----------------|---------|----------|
| **Velocidad** | ~200-500ms | ~1-3 segundos |
| **Gratis/dÃ­a** | 14,400 requests | SegÃºn plan |
| **Modelo** | Llama 3.3 70B | DeepSeek-Chat |
| **Calidad** | â­â­â­â­â­ | â­â­â­â­ |

ğŸ’¡ **RecomendaciÃ³n**: Usa **Groq** para velocidad Ã³ptima (10-20x mÃ¡s rÃ¡pido) con tier gratuito generoso.

### 5. Preparar documentos

Coloca tus archivos `.md` en la carpeta `data/docs/`:

```bash
mkdir -p data/docs
# Copia tus archivos .md a data/docs/
```

## ğŸ“– Uso

### Ingestion de Documentos

Procesa los archivos markdown y genera sus embeddings:

```bash
# Ingestion bÃ¡sica
python src/main.py --ingest

# Ingestion dividiendo documentos en chunks
python src/main.py --ingest --chunk

# Forzar re-procesamiento de documentos existentes
python src/main.py --ingest --force

# Usar DeepSeek en lugar de Groq
python src/main.py --ingest --llm-provider deepseek
```

### Consultas

#### Consulta Ãºnica

```bash
python src/main.py --query "Â¿QuÃ© es Python?"
```

#### Consulta con fuentes

```bash
python src/main.py --query "Â¿CÃ³mo funciona el sistema?" --show-sources
```

#### Modo interactivo simple

```bash
python src/main.py
```

### ğŸ¤– Chatbot Interactivo (Recomendado)

Inicia el chatbot interactivo por consola:

```bash
python src/chat.py
```

**âœ¨ CaracterÃ­sticas del Chatbot:**
- ğŸ’¬ Interfaz de chat por consola limpia e intuitiva
- ğŸ§  Mantiene historial de los Ãºltimos 5 mensajes
- ğŸ” Sistema RAG con bÃºsqueda semÃ¡ntica en documentos
- ğŸ“š Muestra fuentes consultadas con scores de similitud
- âš¡ Respuestas ultra-rÃ¡pidas con Groq (200-500ms)
- ğŸ“Š Comandos especiales:
  - `salir` o `exit`: Terminar el chat
  - `limpiar`: Borrar historial de conversaciÃ³n
  - `stats`: Ver estadÃ­sticas del sistema

**Ejemplo de uso:**
```
ğŸ§‘ TÃº: Â¿QuÃ© informaciÃ³n tienes sobre becas?

ğŸ¤– Chatbot: [Respuesta basada en documentos...]

ğŸ“š Fuentes consultadas:
  1. becas.md (similitud: 0.845)
  2. menciones_honorificas.md (similitud: 0.234)

âš¡ Tiempo: 350ms
```

### Opciones avanzadas

```bash
# Recuperar mÃ¡s documentos relevantes
python src/main.py --query "tu pregunta" --top-k 5

# Ajustar temperatura del LLM (0.0 = mÃ¡s determinista, 1.0 = mÃ¡s creativo)
python src/main.py --query "tu pregunta" --temperature 0.5

# CombinaciÃ³n de opciones
python src/main.py --query "tu pregunta" --top-k 5 --temperature 0.7 --show-sources

# Usar DeepSeek en lugar de Groq
python src/main.py --query "tu pregunta" --llm-provider deepseek
python src/chat.py --llm-provider deepseek
```

### EstadÃ­sticas

Ver informaciÃ³n del sistema:

```bash
python src/main.py --stats
```

### Limpiar base de datos

Eliminar todos los documentos:

```bash
python src/main.py --reset
```

## ğŸ”§ Arquitectura TÃ©cnica

### Pipeline de Ingestion

1. **Carga de archivos**: Lee archivos `.md` desde `data/docs/`
2. **Preprocesamiento**: Limpia el texto (espacios, saltos de lÃ­nea)
3. **Chunking** (opcional): Divide documentos largos en segmentos
4. **GeneraciÃ³n de embeddings**: BGE-M3 crea vectores de 1024 dimensiones (float32)
5. **Almacenamiento**: Guarda en ChromaDB con persistencia automÃ¡tica

### Pipeline de Consulta

1. **Embedding de consulta**: Convierte la pregunta en vector (1024-dim)
2. **BÃºsqueda HNSW**: ChromaDB busca documentos similares con cosine similarity
3. **Ranking**: Ordena por relevancia y selecciona top-k
4. **GeneraciÃ³n RAG**: EnvÃ­a contexto + pregunta a Groq/DeepSeek
5. **Respuesta**: Retorna respuesta basada en contexto

### ChromaDB - Vector Database

**Por quÃ© ChromaDB?**
- âœ… Zero configuraciÃ³n requerida - no necesita servidor
- âœ… DiseÃ±ado especÃ­ficamente para embeddings
- âœ… Algoritmo HNSW (Hierarchical Navigable Small World) para bÃºsqueda rÃ¡pida
- âœ… Persistencia automÃ¡tica a disco
- âœ… Metadata integrada con vectores
- âœ… Excelente para desarrollo y producciÃ³n
- âœ… Base de datos embebida - sin procesos externos

**Detalles tÃ©cnicos:**
- **UbicaciÃ³n**: `data/chroma/` (creado automÃ¡ticamente)
- **ColecciÃ³n**: `documents`
- **MÃ©trica**: Cosine similarity
- **Ãndice**: HNSW
- **Dimensiones**: 1024 (BGE-M3)

## ğŸ§ª Testing de MÃ³dulos Individuales

Cada mÃ³dulo puede ejecutarse de forma independiente para testing:

```bash
# Test de embeddings
python src/embeddings/embedder.py

# Test de ChromaDB
python src/database/chroma_vector_store.py

# Test de ingestion
python src/ingestion/ingest_docs.py

# Test de Groq client
python src/llm/groq_client.py

# Test de DeepSeek client
python src/llm/deepseek_client.py

# Test de retriever
python src/rag/retriever.py

# Test de chatbot
python src/chatbot/chatbot.py
```

## âš ï¸ Requisitos del Sistema

- **Python**: 3.8 o superior
- **RAM**: MÃ­nimo 4GB (recomendado 8GB para BGE-M3)
- **Espacio en disco**: ~2GB para el modelo BGE-M3
- **Internet**: Solo para primera descarga del modelo y llamadas API

**No se requiere:**
- âŒ SQL Server
- âŒ ODBC Drivers
- âŒ ConfiguraciÃ³n de base de datos
- âŒ Servidor externo

## ğŸ› Troubleshooting

### Error: "GROQ_API_KEY no estÃ¡ configurada"

- AsegÃºrate de tener el archivo `.env` en la raÃ­z del proyecto
- Verifica que la API key sea vÃ¡lida
- Copia `.env.example` a `.env` si no existe

### Error: "No hay documentos en la base de datos"

- Ejecuta primero `python src/main.py --ingest`
- Verifica que haya archivos `.md` en `data/docs/`

### El modelo BGE-M3 se descarga muy lento

- El modelo pesa ~2GB, la primera vez tomarÃ¡ tiempo
- Se descarga automÃ¡ticamente en `~/.cache/huggingface/`
- Solo se descarga una vez

### Errores de memoria con BGE-M3

- Cierra otras aplicaciones
- Reduce el tamaÃ±o de los documentos usando `--chunk`

### ChromaDB: Error de persistencia

- Elimina la carpeta `data/chroma/` y vuelve a ejecutar `--ingest`
- Verifica permisos de escritura en `data/`

## ğŸ“ Ejemplo de Uso Completo

```bash
# 1. Configurar entorno
cp .env.example .env
# Editar .env con tu GROQ_API_KEY

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Agregar documentos
echo "# Python\nPython es un lenguaje de programaciÃ³n." > data/docs/python.md
echo "# SQL\nSQL es un lenguaje de consultas." > data/docs/sql.md

# 4. Ingerir documentos
python src/main.py --ingest

# 5. Iniciar chatbot interactivo
python src/chat.py

# O hacer consultas directas
python src/main.py --query "Â¿QuÃ© es Python?" --show-sources
```

## ğŸš€ Ventajas de esta ImplementaciÃ³n

**Vs SQL Server:**
- âœ… Sin instalaciÃ³n ni configuraciÃ³n de base de datos
- âœ… BÃºsqueda vectorial nativa (HNSW)
- âœ… MÃ¡s rÃ¡pido para similitud de embeddings
- âœ… Persistencia automÃ¡tica

**Groq API:**
- âš¡ 10-20x mÃ¡s rÃ¡pido que alternativas
- ğŸ’° 14,400 requests gratis por dÃ­a
- ğŸ¯ Modelo Llama 3.3 70B de alta calidad
- ğŸ”„ FÃ¡cil cambio a DeepSeek si lo necesitas

**BGE-M3:**
- ğŸŒ Modelo multilingÃ¼e (espaÃ±ol, inglÃ©s, etc.)
- ğŸ“Š 1024 dimensiones (buen balance)
- ğŸ¯ Estado del arte en embeddings
- ğŸ†“ Completamente gratuito

## ğŸ¤ Contribuciones

Este es un proyecto de referencia. SiÃ©ntete libre de modificarlo segÃºn tus necesidades.

## ğŸ“„ Licencia

MIT License - SiÃ©ntete libre de usar este cÃ³digo.

## ğŸ”— Enlaces Ãštiles

- [BGE-M3 en Hugging Face](https://huggingface.co/BAAI/bge-m3)
- [Groq Console](https://console.groq.com/)
- [DeepSeek Platform](https://platform.deepseek.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [DocumentaciÃ³n de Sentence Transformers](https://www.sbert.net/)
